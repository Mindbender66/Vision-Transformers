{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk-zvdUdOn1j",
        "outputId": "834e16d9-45a5-4885-e077-c1ab5ffba7a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReAXnu_wOoxg",
        "outputId": "e87a742e-af53-43e0-a706-7c40d412e1f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.2)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.21.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "! pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DA1TNIXGO2_N",
        "outputId": "3af22bb5-3641-4281-db70-ea0a4bc11b57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from PIL import Image, ImageEnhance\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Jmak2MNbO9UK"
      },
      "outputs": [],
      "source": [
        "labels = {'atrofi':0,'iskemi':1,'normal':2,'WMD':3}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xNhMycCPKQv",
        "outputId": "ec976a85-dcd1-44c8-ead4-54f504d10f15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 264/264 [00:04<00:00, 58.95it/s] \n",
            "100%|██████████| 72/72 [00:30<00:00,  2.38it/s]\n",
            "100%|██████████| 1756/1756 [00:39<00:00, 44.03it/s] \n",
            "100%|██████████| 36/36 [00:15<00:00,  2.38it/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "\n",
        "# Define the image size\n",
        "image_size = 150\n",
        "\n",
        "# Define the path to your dataset\n",
        "dataset_path = '/content/drive/MyDrive/train'\n",
        "\n",
        "# Create empty lists to store data and labels\n",
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "# Define your labels dictionary (if you haven't already)\n",
        "labels = {'atrofi':0,'iskemi':1,'normal':2,'WMD':3}  # Update with your actual labels # atrophy,ischemia\n",
        "\n",
        "# Iterate through the labels\n",
        "for label, label_id in labels.items():\n",
        "    label_path = os.path.join(dataset_path, label)\n",
        "\n",
        "    # Iterate through the image files in the current label directory\n",
        "    for image_path in tqdm(glob.glob(os.path.join(label_path, '*.jpg'))):\n",
        "        img = cv2.imread(image_path)\n",
        "\n",
        "        # Resize the image to the desired size\n",
        "        img = cv2.resize(img, (image_size, image_size))\n",
        "\n",
        "        # Append the image to x_train and its corresponding label to y_train\n",
        "        x_train.append(img)\n",
        "        y_train.append(label_id)\n",
        "\n",
        "# Convert lists to NumPy arrays for better performance\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uwOOxnhPNdK",
        "outputId": "82520eb7-2b1f-470c-ddd7-7d72c0240204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 264/264 [00:03<00:00, 81.55it/s] \n",
            "100%|██████████| 72/72 [00:20<00:00,  3.58it/s]\n",
            "100%|██████████| 1749/1749 [00:40<00:00, 43.35it/s] \n",
            "100%|██████████| 36/36 [00:10<00:00,  3.58it/s]\n"
          ]
        }
      ],
      "source": [
        "# Define the path to your dataset\n",
        "dataset_path = '/content/drive/MyDrive/test'\n",
        "\n",
        "# Create empty lists to store data and labels\n",
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "# Define your labels dictionary (if you haven't already)\n",
        "labels = {'atrofi':0,'iskemi':1,'normal':2,'WMD':3}  # Update with your actual labels\n",
        "\n",
        "# Iterate through the labels\n",
        "for label, label_id in labels.items():\n",
        "    label_path = os.path.join(dataset_path, label)\n",
        "\n",
        "    # Iterate through the image files in the current label directory\n",
        "    for image_path in tqdm(glob.glob(os.path.join(label_path, '*.jpg'))):\n",
        "        img = cv2.imread(image_path)\n",
        "\n",
        "        # Resize the image to the desired size\n",
        "        img = cv2.resize(img, (image_size, image_size))\n",
        "\n",
        "        # Append the image to x_train and its corresponding label to y_train\n",
        "        x_train.append(img)\n",
        "        y_train.append(label_id)\n",
        "\n",
        "# Convert lists to NumPy arrays for better performance\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "b8oI8c30PnwJ"
      },
      "outputs": [],
      "source": [
        "x_train, y_train = shuffle(x_train,y_train, random_state=101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rUC0qTldP8o-"
      },
      "outputs": [],
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(x_train,y_train, test_size=0.1,random_state=101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jh8cdp-yP_FU",
        "outputId": "10e0e287-395a-4d78-c0e6-76f1df31f2c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 2 2 ... 2 2 2]\n"
          ]
        }
      ],
      "source": [
        "print(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "A6lr5fHUQA93"
      },
      "outputs": [],
      "source": [
        "# DATA\n",
        "NUM_CLASSES = 4\n",
        "INPUT_SHAPE = (32, 32, 3)\n",
        "BUFFER_SIZE = 512\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "# AUGMENTATION\n",
        "IMAGE_SIZE = 72\n",
        "PATCH_SIZE = 6\n",
        "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
        "\n",
        "# OPTIMIZER\n",
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 0.0001\n",
        "\n",
        "# TRAINING\n",
        "EPOCHS = 60\n",
        "\n",
        "# ARCHITECTURE\n",
        "LAYER_NORM_EPS = 1e-6\n",
        "TRANSFORMER_LAYERS = 8\n",
        "PROJECTION_DIM = 64\n",
        "NUM_HEADS = 4\n",
        "TRANSFORMER_UNITS = [\n",
        "    PROJECTION_DIM * 2,\n",
        "    PROJECTION_DIM,\n",
        "]\n",
        "MLP_HEAD_UNITS = [\n",
        "    2048,\n",
        "    1024\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Flwpm-lNQDZr"
      },
      "outputs": [],
      "source": [
        "train_data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(factor=0.02),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"train_data_augmentation\",\n",
        ")\n",
        "\n",
        "test_data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    ],\n",
        "    name=\"test_data_augmentation\",\n",
        ")\n",
        "\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "train_data_augmentation.layers[0].adapt(x_train)\n",
        "test_data_augmentation.layers[0].adapt(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZGZ6MoY4QFWM"
      },
      "outputs": [],
      "source": [
        "def map_fn_train(image, label):\n",
        "    return (train_data_augmentation(image), label)\n",
        "\n",
        "def map_fn_test(image, label):\n",
        "    return (test_data_augmentation(image), label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "s_ZPn-5MQHGe"
      },
      "outputs": [],
      "source": [
        "AUTO = tf.data.AUTOTUNE\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).map(map_fn_train).prefetch(AUTO)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_ds = test_ds.batch(BATCH_SIZE).map(map_fn_test).prefetch(AUTO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "C6acCec0QJUZ"
      },
      "outputs": [],
      "source": [
        "class ShiftedPatchTokenization(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size=IMAGE_SIZE,\n",
        "        patch_size=PATCH_SIZE,\n",
        "        num_patches=NUM_PATCHES,\n",
        "        projection_dim=PROJECTION_DIM,\n",
        "        vanilla=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.vanilla = vanilla # Flag to swtich to vanilla patch extractor\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.half_patch = patch_size // 2\n",
        "        self.flatten_patches = layers.Reshape((num_patches, -1))\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.layer_norm = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"vanilla\": self.vanilla,\n",
        "            \"image_size\": self.image_size,\n",
        "            \"patch_size\": self.patch_size,\n",
        "            \"half_patch\": self.half_patch\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def crop_shift_pad(self, images, mode):\n",
        "        # Build the diagonally shifted images\n",
        "        if mode == \"left-up\":\n",
        "            crop_height = self.half_patch\n",
        "            crop_width = self.half_patch\n",
        "            shift_height = 0\n",
        "            shift_width = 0\n",
        "        elif mode == \"left-down\":\n",
        "            crop_height = 0\n",
        "            crop_width = self.half_patch\n",
        "            shift_height = self.half_patch\n",
        "            shift_width = 0\n",
        "        elif mode == \"right-up\":\n",
        "            crop_height = self.half_patch\n",
        "            crop_width = 0\n",
        "            shift_height = 0\n",
        "            shift_width = self.half_patch\n",
        "        else:\n",
        "            crop_height = 0\n",
        "            crop_width = 0\n",
        "            shift_height = self.half_patch\n",
        "            shift_width = self.half_patch\n",
        "\n",
        "        # Crop the shifted images and pad them\n",
        "        crop = tf.image.crop_to_bounding_box(\n",
        "            images,\n",
        "            offset_height=crop_height,\n",
        "            offset_width=crop_width,\n",
        "            target_height=self.image_size-self.half_patch,\n",
        "            target_width=self.image_size-self.half_patch,\n",
        "        )\n",
        "        shift_pad = tf.image.pad_to_bounding_box(\n",
        "            crop,\n",
        "            offset_height=shift_height,\n",
        "            offset_width=shift_width,\n",
        "            target_height=self.image_size,\n",
        "            target_width=self.image_size,\n",
        "        )\n",
        "        return shift_pad\n",
        "\n",
        "    def call(self, images):\n",
        "        if not self.vanilla:\n",
        "            # Concat the shifted images with the original image\n",
        "            images = tf.concat(\n",
        "                [\n",
        "                    images,\n",
        "                    self.crop_shift_pad(images, mode=\"left-up\"),\n",
        "                    self.crop_shift_pad(images, mode=\"left-down\"),\n",
        "                    self.crop_shift_pad(images, mode=\"right-up\"),\n",
        "                    self.crop_shift_pad(images, mode=\"right-down\"),\n",
        "                ],\n",
        "                axis=-1\n",
        "            )\n",
        "        # Patchify the images and flatten it\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        flat_patches = self.flatten_patches(patches)\n",
        "        if not self.vanilla:\n",
        "            # Layer normalize the flat patches and linearly project it\n",
        "            tokens = self.layer_norm(flat_patches)\n",
        "            tokens = self.projection(tokens)\n",
        "        else:\n",
        "            # Linearly project the flat patches\n",
        "            tokens = self.projection(flat_patches)\n",
        "        return (tokens, patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mP-1MrdkQK3s"
      },
      "outputs": [],
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches=NUM_PATCHES, projection_dim=PROJECTION_DIM):\n",
        "        super().__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "        self.positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"num_patches\": self.num_patches,\n",
        "            \"positions\": self.positions.numpy(),\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def call(self, encoded_patches):\n",
        "        encoded_positions = self.position_embedding(self.positions)\n",
        "        encoded_patches = encoded_patches + encoded_positions\n",
        "        return encoded_patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "bggofav-QPlX"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttentionLSA(tf.keras.layers.MultiHeadAttention):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # The trainable temperature term. The initial value is\n",
        "        # the square root of the key dimension.\n",
        "        self.tau = tf.Variable(\n",
        "            math.sqrt(float(self._key_dim)),\n",
        "            trainable=True\n",
        "        )\n",
        "        # Build the diagonal attention mask\n",
        "        diag_attn_mask = 1 - tf.eye(NUM_PATCHES)\n",
        "        self.diag_attn_mask = tf.cast([diag_attn_mask], dtype=tf.int8)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"tau\": self.tau.numpy(),\n",
        "            \"diag_attn_mask\": self.diag_attn_mask.numpy(),\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def _compute_attention(\n",
        "        self,\n",
        "        query,\n",
        "        key,\n",
        "        value,\n",
        "        attention_mask=None,\n",
        "        training=None\n",
        "    ):\n",
        "        query = tf.multiply(query, 1.0 / self.tau)\n",
        "        attention_scores = tf.einsum(self._dot_product_equation, key, query)\n",
        "        attention_scores = self._masked_softmax(attention_scores, attention_mask=self.diag_attn_mask)\n",
        "        attention_scores_dropout = self._dropout_layer(attention_scores, training=training)\n",
        "        attention_output = tf.einsum(self._combine_equation, attention_scores_dropout, value)\n",
        "        return attention_output, attention_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lR5Z8XG1QRyB"
      },
      "outputs": [],
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EvKy6URIQUDQ"
      },
      "outputs": [],
      "source": [
        "def create_vit_classifier(vanilla=False):\n",
        "    inputs = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3), name=\"input_layer\")\n",
        "    # Create patches.\n",
        "    (tokens, _)  = ShiftedPatchTokenization(vanilla=vanilla)(inputs)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder()(tokens)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(TRANSFORMER_LAYERS):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        if not vanilla:\n",
        "            attention_output = MultiHeadAttentionLSA(\n",
        "                num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n",
        "            )(x1, x1)\n",
        "        else:\n",
        "            attention_output = layers.MultiHeadAttention(\n",
        "                num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n",
        "            )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=TRANSFORMER_UNITS, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=MLP_HEAD_UNITS, dropout_rate=0.5)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(NUM_CLASSES, name=\"output_dense\")(features)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7TqDblePQVeP"
      },
      "outputs": [],
      "source": [
        "class WarmUpCosine(keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(\n",
        "        self, learning_rate_base, total_steps, warmup_learning_rate, warmup_steps\n",
        "    ):\n",
        "        super(WarmUpCosine, self).__init__()\n",
        "\n",
        "        self.learning_rate_base = learning_rate_base\n",
        "        self.total_steps = total_steps\n",
        "        self.warmup_learning_rate = warmup_learning_rate\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.pi = tf.constant(np.pi)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        if self.total_steps < self.warmup_steps:\n",
        "            raise ValueError(\"Total_steps must be larger or equal to warmup_steps.\")\n",
        "\n",
        "        cos_annealed_lr = tf.cos(\n",
        "            self.pi\n",
        "            * (tf.cast(step, tf.float32) - self.warmup_steps)\n",
        "            / float(self.total_steps - self.warmup_steps)\n",
        "        )\n",
        "        learning_rate = 0.5 * self.learning_rate_base * (1 + cos_annealed_lr)\n",
        "\n",
        "        if self.warmup_steps > 0:\n",
        "            if self.learning_rate_base < self.warmup_learning_rate:\n",
        "                raise ValueError(\n",
        "                    \"Learning_rate_base must be larger or equal to \"\n",
        "                    \"warmup_learning_rate.\"\n",
        "                )\n",
        "            slope = (\n",
        "                self.learning_rate_base - self.warmup_learning_rate\n",
        "            ) / self.warmup_steps\n",
        "            warmup_rate = slope * tf.cast(step, tf.float32) + self.warmup_learning_rate\n",
        "            learning_rate = tf.where(\n",
        "                step < self.warmup_steps, warmup_rate, learning_rate\n",
        "            )\n",
        "        return tf.where(\n",
        "            step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kHSgamYCQWy0"
      },
      "outputs": [],
      "source": [
        "def run_experiment(model):\n",
        "    total_steps = int((len(x_train) / BATCH_SIZE) * EPOCHS)\n",
        "    warmup_epoch_percentage = 0.10\n",
        "    warmup_steps = int(total_steps * warmup_epoch_percentage)\n",
        "    scheduled_lrs = WarmUpCosine(\n",
        "        learning_rate_base=LEARNING_RATE,\n",
        "        total_steps=total_steps,\n",
        "        warmup_learning_rate=0.0,\n",
        "        warmup_steps=warmup_steps,\n",
        "    )\n",
        "\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        epochs=EPOCHS\n",
        "    )\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(test_ds)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpbpmg2lQYLn",
        "outputId": "20a76415-30e2-4f39-a07c-9f0b436c5f52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "8/8 [==============================] - 204s 23s/step - loss: 4.4904 - accuracy: 0.6719 - top-5-accuracy: 1.0000\n",
            "Epoch 2/60\n",
            "8/8 [==============================] - 183s 23s/step - loss: 0.7724 - accuracy: 0.7668 - top-5-accuracy: 1.0000\n",
            "Epoch 3/60\n",
            "8/8 [==============================] - 174s 21s/step - loss: 0.6760 - accuracy: 0.8134 - top-5-accuracy: 1.0000\n",
            "Epoch 4/60\n",
            "8/8 [==============================] - 173s 21s/step - loss: 0.6611 - accuracy: 0.8181 - top-5-accuracy: 1.0000\n",
            "Epoch 5/60\n",
            "8/8 [==============================] - 181s 22s/step - loss: 0.6403 - accuracy: 0.8134 - top-5-accuracy: 1.0000\n",
            "Epoch 6/60\n",
            "8/8 [==============================] - 170s 21s/step - loss: 0.6094 - accuracy: 0.8239 - top-5-accuracy: 1.0000\n",
            "Epoch 7/60\n",
            "8/8 [==============================] - 166s 20s/step - loss: 0.5944 - accuracy: 0.8197 - top-5-accuracy: 1.0000\n",
            "Epoch 8/60\n",
            "8/8 [==============================] - 169s 20s/step - loss: 0.5878 - accuracy: 0.8218 - top-5-accuracy: 1.0000\n",
            "Epoch 9/60\n",
            "8/8 [==============================] - 168s 21s/step - loss: 0.5911 - accuracy: 0.8260 - top-5-accuracy: 1.0000\n",
            "Epoch 10/60\n",
            "8/8 [==============================] - 168s 21s/step - loss: 0.5622 - accuracy: 0.8229 - top-5-accuracy: 1.0000\n",
            "Epoch 11/60\n",
            "8/8 [==============================] - 164s 20s/step - loss: 0.5552 - accuracy: 0.8302 - top-5-accuracy: 1.0000\n",
            "Epoch 12/60\n",
            "8/8 [==============================] - 162s 20s/step - loss: 0.5657 - accuracy: 0.8202 - top-5-accuracy: 1.0000\n",
            "Epoch 13/60\n",
            "8/8 [==============================] - 145s 18s/step - loss: 0.5496 - accuracy: 0.8328 - top-5-accuracy: 1.0000\n",
            "Epoch 14/60\n",
            "8/8 [==============================] - 155s 19s/step - loss: 0.5325 - accuracy: 0.8297 - top-5-accuracy: 1.0000\n",
            "Epoch 15/60\n",
            "8/8 [==============================] - 166s 20s/step - loss: 0.5535 - accuracy: 0.8333 - top-5-accuracy: 1.0000\n",
            "Epoch 16/60\n",
            "8/8 [==============================] - 155s 19s/step - loss: 0.4999 - accuracy: 0.8349 - top-5-accuracy: 1.0000\n",
            "Epoch 17/60\n",
            "8/8 [==============================] - 151s 19s/step - loss: 0.5010 - accuracy: 0.8407 - top-5-accuracy: 1.0000\n",
            "Epoch 18/60\n",
            "8/8 [==============================] - 153s 19s/step - loss: 0.4975 - accuracy: 0.8375 - top-5-accuracy: 1.0000\n",
            "Epoch 19/60\n",
            "8/8 [==============================] - 151s 19s/step - loss: 0.5151 - accuracy: 0.8333 - top-5-accuracy: 1.0000\n",
            "Epoch 20/60\n",
            "8/8 [==============================] - 159s 19s/step - loss: 0.4836 - accuracy: 0.8412 - top-5-accuracy: 1.0000\n",
            "Epoch 21/60\n",
            "8/8 [==============================] - 157s 19s/step - loss: 0.4655 - accuracy: 0.8470 - top-5-accuracy: 1.0000\n",
            "Epoch 22/60\n",
            "8/8 [==============================] - 152s 19s/step - loss: 0.4720 - accuracy: 0.8401 - top-5-accuracy: 1.0000\n",
            "Epoch 23/60\n",
            "8/8 [==============================] - 155s 19s/step - loss: 0.4669 - accuracy: 0.8438 - top-5-accuracy: 1.0000\n",
            "Epoch 24/60\n",
            "8/8 [==============================] - 154s 19s/step - loss: 0.4629 - accuracy: 0.8433 - top-5-accuracy: 1.0000\n",
            "Epoch 25/60\n",
            "8/8 [==============================] - 165s 20s/step - loss: 0.4599 - accuracy: 0.8449 - top-5-accuracy: 1.0000\n",
            "Epoch 26/60\n",
            "8/8 [==============================] - 157s 19s/step - loss: 0.4632 - accuracy: 0.8527 - top-5-accuracy: 1.0000\n",
            "Epoch 27/60\n",
            "8/8 [==============================] - 155s 19s/step - loss: 0.4362 - accuracy: 0.8585 - top-5-accuracy: 1.0000\n",
            "Epoch 28/60\n",
            "8/8 [==============================] - 154s 19s/step - loss: 0.4450 - accuracy: 0.8522 - top-5-accuracy: 1.0000\n",
            "Epoch 29/60\n",
            "8/8 [==============================] - 162s 20s/step - loss: 0.4456 - accuracy: 0.8480 - top-5-accuracy: 1.0000\n",
            "Epoch 30/60\n",
            "8/8 [==============================] - 162s 20s/step - loss: 0.4235 - accuracy: 0.8543 - top-5-accuracy: 1.0000\n",
            "Epoch 31/60\n",
            "8/8 [==============================] - 161s 20s/step - loss: 0.4436 - accuracy: 0.8506 - top-5-accuracy: 1.0000\n",
            "Epoch 32/60\n",
            "8/8 [==============================] - 168s 21s/step - loss: 0.4299 - accuracy: 0.8496 - top-5-accuracy: 1.0000\n",
            "Epoch 33/60\n",
            "8/8 [==============================] - 167s 20s/step - loss: 0.4065 - accuracy: 0.8595 - top-5-accuracy: 1.0000\n",
            "Epoch 34/60\n",
            "8/8 [==============================] - 156s 19s/step - loss: 0.4265 - accuracy: 0.8543 - top-5-accuracy: 1.0000\n",
            "Epoch 35/60\n",
            "8/8 [==============================] - 162s 20s/step - loss: 0.4150 - accuracy: 0.8606 - top-5-accuracy: 1.0000\n",
            "Epoch 36/60\n",
            "8/8 [==============================] - 158s 20s/step - loss: 0.3960 - accuracy: 0.8674 - top-5-accuracy: 1.0000\n",
            "Epoch 37/60\n",
            "8/8 [==============================] - 159s 19s/step - loss: 0.3934 - accuracy: 0.8658 - top-5-accuracy: 1.0000\n",
            "Epoch 38/60\n",
            "8/8 [==============================] - 155s 19s/step - loss: 0.3953 - accuracy: 0.8616 - top-5-accuracy: 1.0000\n",
            "Epoch 39/60\n",
            "8/8 [==============================] - 160s 20s/step - loss: 0.3935 - accuracy: 0.8653 - top-5-accuracy: 1.0000\n",
            "Epoch 40/60\n",
            "8/8 [==============================] - 156s 19s/step - loss: 0.3904 - accuracy: 0.8611 - top-5-accuracy: 1.0000\n",
            "Epoch 41/60\n",
            "8/8 [==============================] - 154s 19s/step - loss: 0.3895 - accuracy: 0.8643 - top-5-accuracy: 1.0000\n",
            "Epoch 42/60\n",
            "8/8 [==============================] - 157s 19s/step - loss: 0.3874 - accuracy: 0.8669 - top-5-accuracy: 1.0000\n",
            "Epoch 43/60\n",
            "8/8 [==============================] - 156s 19s/step - loss: 0.3899 - accuracy: 0.8732 - top-5-accuracy: 1.0000\n",
            "Epoch 44/60\n",
            "8/8 [==============================] - 159s 19s/step - loss: 0.3913 - accuracy: 0.8627 - top-5-accuracy: 1.0000\n",
            "Epoch 45/60\n",
            "8/8 [==============================] - 158s 20s/step - loss: 0.3701 - accuracy: 0.8758 - top-5-accuracy: 1.0000\n",
            "Epoch 46/60\n",
            "8/8 [==============================] - 158s 20s/step - loss: 0.3532 - accuracy: 0.8774 - top-5-accuracy: 1.0000\n",
            "Epoch 47/60\n",
            "8/8 [==============================] - 155s 19s/step - loss: 0.3802 - accuracy: 0.8705 - top-5-accuracy: 1.0000\n",
            "Epoch 48/60\n",
            "8/8 [==============================] - 158s 19s/step - loss: 0.3628 - accuracy: 0.8800 - top-5-accuracy: 1.0000\n",
            "Epoch 49/60\n",
            "8/8 [==============================] - 158s 20s/step - loss: 0.3465 - accuracy: 0.8826 - top-5-accuracy: 1.0000\n",
            "Epoch 50/60\n",
            "8/8 [==============================] - 152s 19s/step - loss: 0.3320 - accuracy: 0.8842 - top-5-accuracy: 1.0000\n",
            "Epoch 51/60\n",
            "8/8 [==============================] - 147s 18s/step - loss: 0.3598 - accuracy: 0.8742 - top-5-accuracy: 1.0000\n",
            "Epoch 52/60\n",
            "8/8 [==============================] - 144s 18s/step - loss: 0.3492 - accuracy: 0.8774 - top-5-accuracy: 1.0000\n",
            "Epoch 53/60\n",
            "8/8 [==============================] - 142s 18s/step - loss: 0.3385 - accuracy: 0.8889 - top-5-accuracy: 1.0000\n",
            "Epoch 54/60\n",
            "8/8 [==============================] - 140s 17s/step - loss: 0.3454 - accuracy: 0.8805 - top-5-accuracy: 1.0000\n",
            "Epoch 55/60\n",
            "8/8 [==============================] - 144s 18s/step - loss: 0.3286 - accuracy: 0.8821 - top-5-accuracy: 1.0000\n",
            "Epoch 56/60\n",
            "8/8 [==============================] - 143s 18s/step - loss: 0.3281 - accuracy: 0.8847 - top-5-accuracy: 1.0000\n",
            "Epoch 57/60\n",
            "8/8 [==============================] - 144s 18s/step - loss: 0.3211 - accuracy: 0.8905 - top-5-accuracy: 1.0000\n",
            "Epoch 58/60\n",
            "8/8 [==============================] - 146s 18s/step - loss: 0.3312 - accuracy: 0.8847 - top-5-accuracy: 1.0000\n",
            "Epoch 59/60\n",
            "8/8 [==============================] - 141s 17s/step - loss: 0.3360 - accuracy: 0.8795 - top-5-accuracy: 1.0000\n",
            "Epoch 60/60\n",
            "8/8 [==============================] - 147s 18s/step - loss: 0.3160 - accuracy: 0.8826 - top-5-accuracy: 1.0000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.3634 - accuracy: 0.8685 - top-5-accuracy: 1.0000\n",
            "Test accuracy: 86.85%\n",
            "Test top 5 accuracy: 100.0%\n"
          ]
        }
      ],
      "source": [
        "vit_sl = create_vit_classifier(vanilla=False)\n",
        "history = run_experiment(vit_sl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0foJ69QB4wgE"
      },
      "source": [
        "# With google downloaded data set"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}